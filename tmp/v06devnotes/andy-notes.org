OVERALL NOTE: I've been using the '-tidy' addition to the filenames
for all this, so I don't screw with any code that might be dependent
on Lenard's original version.

* DONE Code formatting (including .Rd)
In catlearn, we try to keep to the GNU standard of max. 79 character column
width. This ensures readibility in the widest range of environments. I've fixed
this as I've gone along. How easy this is to achieve depends a bit on your text
editor. It's easy with Emacs, which is not surprising as Emacs is GNU. But I
imagine it's possible in most text editors.
* DONE Programming style
- Avoid passing things to functions that they don't use e.g. mu.pos in
  .cluster.activation. I've fixed this.

- Also try to avoid passing things to functions that the functions can easily
  calculate for themselves e.g. 'e', 'fac.na', which I've now fixed. Avoiding
  this makes the code more modular and hence easier to maintain. I've fixed this.

- The function exp(x) is a better choice that defining e <- exp(1) and then
  doing e^x (i.e. pretty sure it runs faster). I've fixed this.

- Try to avoid code repetition, it makes the code harder to debug and
  maintain (this was mainly an issue in the cluster recruitment
  section, and I fixed it).

- Avoid querying column names (e.g. which (colnames(tr) == "t") )
  because this functionality is not easily available within a later C++
  implementation.

-  c.act$act[which.max(c.act$act)] is equiv. to max(c.act$act), so I
   replaced with this simpler form for clarity

- Where possible, pre-define arrays to the size you know they will
  become (this makes the code run faster). This is tricky for things
  like cluster and w, because the model is dynamic, but can be easily
  done for e.g. xout stuff. I have fixed this where it was easy to do so.

* DONE Rd file

I appreciate these are not quite finished, but I'm afraid I needed to read them
anyway in order to be able to make  sense of the code.

I tidied up the phrasing a little bit as I went along. There were quite a few
bits where I felt the user would want more information - I've inserted some
comments to indicate what and where.

* DONE slpSUSTAIN error in code
- The Eq.10 - in your code, if all units have the same activation,
  this breaks the tie by going for the unit corresponding to the
  correct answer (which will therefore not result in a new cluster
  being formed). This seems different to the following:

  "the output unit representing the correct nominal value must be the
  most activated of all the output units forming the queried stimulus
  dimension" (Love et al., 2004, p. 315)

I think you have this because of the edge case where the system has
been reset, but you can more easily detect that with ctrl.


- The Eq. 14 implementation is wrong -- it updates all weights, but the
  paper says to update weights to queried dimension only. I've fixed this.

* TODO slpSUSTAIN requested changes
** DONE Fix colskip
- Colskip operates differently to all other slp modules.
** DONE Fix t in tr
- Better not use 't' for supervised/unsupervised. This is because, in
  all other slp* models, t indicates category membership. More
  generally, we probably want to use ctrl rather than another column
  for this.
** DONE Make it stateful
- slpSUSTAIN is not stateful. Specficially, it does not take in the
  'cluster' matrix as part of 'st'. Also, 'w' in 'st' is a vector,
  while for slpSUSTAIN to be stateful, 'w' would have to be a matrix
  (one row for each cluster). I couldn't really see the point of
  having a 'w' vector in st - what would be the meaning of non-zero
  values in this vector, given they are not associated with any
  particular cluster?

** DONE Drop 'mode' etc. from  output. 
In catlearn (like UNIX more generally) we try to keep commands doing
one thing well. This modularity makes it much easier to maintain and
debug code. So, things like 'mode', which are essentially analysis of
the model output, shouldn't be in the same command (slpSUSTAIN) as
actually running the model.

** TODO Queried dimension chooseable
- The queried dimension is always the last one. The user should be
  able to choose which dimension to query.

* TODO slpSUSTAIN queries
- I couldn't see any point where the following code would be used, so I
  dropped it:

    if(length(fac.queried) == 0){
        fac.queried <- fac.na
    }

* TODO Simulation Nosofsky et al. (1994)
** DONE Fix randomization
You seem to randomize across blocks. Nosofsky et al. (1994) randomized
within blocks, as did Love et al. (2004, see p. 319). There's also a
certain amount of 're-inventing the wheel' here -- the pre-existing
catlearn function nosof94train gets you nearly all the way there
anyway. I've fixed both by updating nosof94train to cope with SUSTAIN
coding and using that.Note that nosof94train by default, represents
the training slightly more accurately than does sustain_py, which
might potentially be a source of differences. I've added an option to
nosof94train so that it can replicate this 'feature' of the sustain_py
simulation.
** DONE Fix initial lambda
In 'st', you defined lambda = 1. I think this needs to be lambda = c(1,1,1),
otherwise your .cluster.activation function gives activations that exceed 1,
which is not permitted under SUSTAIN. I've fixed this.

** NOTE: Difference between their simulation and ours

sustain-python implements the criterion-based early finish of the
experiment (and assumes P(correct response) is 1 after the criterion
is passed, which is what Nosofsky et al. did in their experiment,
too). 

There is no good way to do this without cause problems for parameter
optimization -- because it introduces a random factor (which means
that, even with the same st and tr, two runs of the model will not
give the same answers). 

It's probably not a massive deal, as the probabilities set to 1 would
be close to 1 anyway, but it might mean we don't get exactly the
results as them...

if there are
differences between implementations, this might explain that.

DON'T try and fix this inside slpSUSTAIN. The easiest solution is to
turn the response probability on each trial 

** NOTE: How good can block 1 be?

