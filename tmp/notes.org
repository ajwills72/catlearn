* TODO trial order
- Tom's version randomizes across blocks, while my version randomizes
within blocks. It should not affect the overall outcome, although
LePelley and McLaren randomized within block - just good to know if we
want to include the data set. 
- *I am unsure if it needs a fix, but can explain slight differences*
* NOTE funny solutions
** NOTE replicating matrices
Instead of using the pracma package, I used do.call(rbind, replicate(number,
matrix)), because it is much faster.
** NOTE updating alpha
The way Tom calculated the magnitude of prediction errors for alpha (by
which it is essentially updated) is a bit hard to do for a state list
processor. The reason is that slpMK75 will need to be able to handle
as many cues at the same time as we want. For this reason I picked the
equation presented in Le Pelley et al. (2016). This produces the same
result - I checked it on paper as proof, but we can write a script to
prove it though.
* TODO LePelley and McLaren sim
** DONE -multiple outcomes
In line with other single linear operators implemented in catlearn (RW, BM),
slpMK75 only uses one column for outcomes. I can imagine two ways to solve
this. First, we use -1 instead of 0. It seems to work like a charm. Another
is to run another sim, but for outcome 2. It can be done by tr[, "t"] <- 
abs(tr[, "t"] - 1). I would pick the version with -1, because that is the
computationally less intensive. I chose to interpret outputs of 
association strengths as a distance between two points representing two
different outcomes. This way, association strengths are informative of both
outcomes without the need to update them separately.
** DONE reset weights but not alphas
I think it might be useful if the function can reset weights but not alphas.
For example, there might be a new outcome, where the stimuli remains the
same. In that case, the state last processor can simply reset weight and
start learning. This solution is necessary because of the binary
representation of input and having only one t column. I see no reason to
reset alphas without resetting the weights as well, let me know of you find
one.
Another reason why I added this option is because in the simulation, there
are four outcomes, where 1 and 2 are unique to stage 1, while 3 and 4 are
unique to stage 2. Resetting weights without alphas just makes it easier to
handle it.
* TODO debugging
** DONE Naming things
Naming things is not easy...
function: slpMK75
train of ap: lepelleymclaren2003train
sim: lepelleymclaren2003mk75
** DONE Data frame problems
There was some problem with how I extracted elements of a list as a data
frame and not a numerical matrix. This cause the Rcpp function to end with an
error message: Not compatible with requested type: [type=list; target=double].
This has been fixed and was not related to either tr or slpMK75.
*I added a boolean to output stimuli encodings, so this bug is redundant
now.*
** TODO alpha high-value error
During the second stage, after the first few trials, something happens to the
alphas that causes them to change high values to 0.1 instead of 1. I don't
really know why it is happening, but working on it heavily. The error is
reproducible, so it should not take long to fix it. If anyone is interested
in taking a closer look, look at values from trials 160-165.
#+BEGIN_SRC R
slpMK75(st, lepelleymclaren2003train(), xtdo = TRUE)$xouta[160:165, ]
slpMK75(st, lepelleymclaren2003train(), xtdo = TRUE)$xoutw[160:165, ]
#+END_SRC
Outputs from S1 looks pretty promising though!!
* TODO documentations
** main function
** simulation
** training matrix
